envs:
  train:
    env_id: "Ant-v3"
    n_envs: 4
    seeds: "${range:4}"
    subproc: false
    monitor:
      root_dir: "./monitor"
      video: true
      video_kwargs:
        interval: null
  eval:
    env_id: "Ant-v3"
    n_envs: 1
    seeds: 100
    subproc: false
    monitor:
      root_dir: "./monitor"
      video: true
      video_kwargs:
        interval: 1

trainer:
  gpus: 1
  max_epochs: 1000
  batches_per_epoch: 250
  #strategy: "ddp2"

agent:
  type: "sac"
  multi_step: 1
  reward_scale: 1.0
  policy_update: 1
  n_steps: 4
  warmup_steps: 10000
  batch_size: 256
  value_loss: "l2"
  replay_buffer: '${L:model.replay_buffer}'
  policy_net: '${L:model.policy_net}'
  value_nets: '${L:model.value_net}'
  policy_optim: '${L:model.policy_optim}'
  value_optim: '${L:model.value_optim}'
  alpha_optim: '${L:model.alpha_optim}'


model:
  replay_buffer:
    type: "default"
    size: 1000000
  policy_net:
    type: "default"
    net: "{L:model.policy_base}"
    squash: true
  value_nets:
    type: "default"
    nets:
      - "${L:model.value_base}"
      - "${L:model.value_base}"
    n_heads: 2
  policy_optim:
    type: "adam"
    lr: 3e-4
    weight_decay: null
    clipnorm: null
  value_optim:
    type: "adam"
    lr: 3e-4
    weight_decay: null
    clipnorm: null
  alpha_optim:
    type: "adam"
    lr: "3e-4"
    weight_decay: null
    clipnorm: null
  policy_base:
    type: "mlp"
    units: [256, 256]
    activ: "relu"
  value_base:
    type: "mlp"
    units: [256, 256]
    activ: "relu"